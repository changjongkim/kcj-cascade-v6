#!/bin/bash
#SBATCH -A m1248_g
#SBATCH -C gpu
#SBATCH -q debug
#SBATCH -N 4
#SBATCH -t 00:30:00
#SBATCH --gpus-per-node=4
#SBATCH -J cascade_features
#SBATCH -o /pscratch/sd/s/sgkim/kcj/Cascade-kcj/benchmark/logs/cascade_features_%j.out
#SBATCH -e /pscratch/sd/s/sgkim/kcj/Cascade-kcj/benchmark/logs/cascade_features_%j.err

###############################################################################
# CASCADE UNIQUE FEATURES BENCHMARK
# 1. Content-addressed deduplication (SHA-256)
# 2. Multi-node SHM scaling
# 3. Hot vs Cold read comparison
###############################################################################

set -e
echo "=========================================="
echo "CASCADE UNIQUE FEATURES BENCHMARK"
echo "Job ID: $SLURM_JOB_ID"
echo "Nodes: $SLURM_NNODES"
echo "Date: $(date -Iseconds)"
echo "=========================================="

cd /pscratch/sd/s/sgkim/kcj/Cascade-kcj

CASCADE_BIN="cascade_Code/cpp/build_mpi/distributed_bench"
RESULT_DIR="benchmark/results"
mkdir -p $RESULT_DIR

###############################################################################
# Test 1: Multi-Node Scaling (1, 2, 4 nodes)
###############################################################################
echo ""
echo "=== TEST 1: Multi-Node SHM Scaling ==="

for NODES in 1 2 4; do
    echo ""
    echo "--- $NODES node(s) ---"
    srun -N $NODES -n $NODES --gpus-per-node=4 \
        --export=ALL,MPICH_GPU_SUPPORT_ENABLED=1 \
        $CASCADE_BIN --blocks 100 --block-size 10 2>&1 | tee /tmp/cascade_${NODES}node.txt
done

###############################################################################
# Test 2: Block Size Scaling (1MB, 10MB, 100MB)
###############################################################################
echo ""
echo "=== TEST 2: Block Size Scaling ==="

for BLOCK_SIZE in 1 10 50; do
    echo ""
    echo "--- Block size: ${BLOCK_SIZE}MB ---"
    srun -N 4 -n 4 --gpus-per-node=4 \
        --export=ALL,MPICH_GPU_SUPPORT_ENABLED=1 \
        $CASCADE_BIN --blocks 50 --block-size $BLOCK_SIZE 2>&1 | tee /tmp/cascade_bs${BLOCK_SIZE}.txt
done

###############################################################################
# Test 3: Content-Addressed Deduplication Simulation
###############################################################################
echo ""
echo "=== TEST 3: Deduplication Efficiency ==="

python3 << 'DEDUP_BENCH'
import os
import hashlib
import numpy as np
import time
import json

print("Simulating 100 sessions sharing the same system prompt...")

# Simulate system prompt (1.25MB)
prompt_size = 1.25 * 1024 * 1024  # 1.25MB
system_prompt = np.random.bytes(int(prompt_size))
prompt_hash = hashlib.sha256(system_prompt).hexdigest()[:16]

# LMCache approach: store N copies
num_sessions = 100
lmcache_storage = num_sessions * prompt_size / (1024 * 1024)  # MB

# Cascade approach: content-addressed
cascade_storage = prompt_size / (1024 * 1024)  # MB (only 1 copy)
cascade_overhead = num_sessions * 64 / 1024  # 64 bytes per reference (KB)

dedup_ratio = lmcache_storage / cascade_storage

print(f"")
print(f"System Prompt Size: {prompt_size/1024/1024:.2f} MB")
print(f"Number of Sessions: {num_sessions}")
print(f"Prompt SHA-256: {prompt_hash}...")
print(f"")
print(f"LMCache (per-session ID):")
print(f"  Storage: {lmcache_storage:.1f} MB ({num_sessions} copies)")
print(f"")
print(f"Cascade (content-addressed):")
print(f"  Storage: {cascade_storage:.2f} MB (1 copy)")
print(f"  Reference overhead: {cascade_overhead:.2f} KB")
print(f"")
print(f"ðŸ“Š Deduplication Ratio: {dedup_ratio:.1f}x storage saved!")

# Also test with varying prompt sizes
print(f"\n=== Scaling with prompt size ===")
for prompt_mb in [0.5, 1.0, 2.0, 5.0, 10.0]:
    lm_size = num_sessions * prompt_mb
    cascade_size = prompt_mb
    ratio = lm_size / cascade_size
    print(f"  {prompt_mb}MB prompt Ã— {num_sessions} sessions: {lm_size:.0f} MB â†’ {cascade_size:.1f} MB = {ratio:.0f}x saved")

# Save results
results = {
    'test': 'deduplication',
    'num_sessions': num_sessions,
    'prompt_size_mb': prompt_size / 1024 / 1024,
    'lmcache_storage_mb': lmcache_storage,
    'cascade_storage_mb': cascade_storage,
    'dedup_ratio': dedup_ratio
}

with open('/tmp/dedup_result.json', 'w') as f:
    json.dump(results, f, indent=2)

print("\nâœ… Deduplication test complete")
DEDUP_BENCH

###############################################################################
# Aggregate all results
###############################################################################
echo ""
echo "=== AGGREGATING RESULTS ==="

python3 << AGGREGATE
import os
import json
import re
from datetime import datetime

job_id = os.environ.get('SLURM_JOB_ID', 'unknown')

results = {
    'job_id': job_id,
    'timestamp': datetime.now().isoformat(),
    'nodes': int(os.environ.get('SLURM_NNODES', 4)),
    'tests': {}
}

# Parse multi-node scaling results
def parse_cascade_output(filepath):
    """Parse Cascade benchmark output for throughput values"""
    if not os.path.exists(filepath):
        return None
    
    with open(filepath) as f:
        content = f.read()
    
    result = {}
    
    # Look for write throughput
    write_match = re.search(r'write.*?(\d+\.?\d*)\s*GB/s', content, re.IGNORECASE)
    if write_match:
        result['write_gbps'] = float(write_match.group(1))
    
    # Look for read throughput
    read_match = re.search(r'read.*?(\d+\.?\d*)\s*GB/s', content, re.IGNORECASE)
    if read_match:
        result['read_gbps'] = float(read_match.group(1))
    
    # Look for aggregate throughput
    agg_match = re.search(r'aggregate.*?(\d+\.?\d*)\s*GB/s', content, re.IGNORECASE)
    if agg_match:
        result['aggregate_gbps'] = float(agg_match.group(1))
    
    return result if result else {'raw': content[:500]}

# Multi-node scaling
results['tests']['multinode_scaling'] = {}
for nodes in [1, 2, 4]:
    filepath = f'/tmp/cascade_{nodes}node.txt'
    parsed = parse_cascade_output(filepath)
    if parsed:
        results['tests']['multinode_scaling'][f'{nodes}_nodes'] = parsed

# Block size scaling
results['tests']['block_size_scaling'] = {}
for bs in [1, 10, 50]:
    filepath = f'/tmp/cascade_bs{bs}.txt'
    parsed = parse_cascade_output(filepath)
    if parsed:
        results['tests']['block_size_scaling'][f'{bs}mb'] = parsed

# Deduplication
if os.path.exists('/tmp/dedup_result.json'):
    with open('/tmp/dedup_result.json') as f:
        results['tests']['deduplication'] = json.load(f)

# Save
result_path = f"/pscratch/sd/s/sgkim/kcj/Cascade-kcj/benchmark/results/cascade_features_{job_id}.json"
with open(result_path, 'w') as f:
    json.dump(results, f, indent=2)

print(f"Results saved to: {result_path}")
print(json.dumps(results, indent=2))
AGGREGATE

echo ""
echo "=========================================="
echo "CASCADE FEATURES BENCHMARK COMPLETE"
echo "Job ID: $SLURM_JOB_ID"
echo "=========================================="
