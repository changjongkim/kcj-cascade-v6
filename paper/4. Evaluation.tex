\section{Evaluation}
\label{sec:evaluation}

%==============================================================================
% ðŸš¨ WARNING: THIS SECTION NEEDS REAL BENCHMARK RESULTS
%==============================================================================

\begin{tcolorbox}[colback=red!10,colframe=red!75!black,title=TODO: Real Benchmark Results Required]
\textbf{This section is a placeholder.}
All benchmarks must be run using actual third\_party implementations:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{LMCache}: \texttt{third\_party/LMCache/lmcache/v1/storage\_backend/local\_disk\_backend.py}
    \item \textbf{PDC}: \texttt{third\_party/pdc/install/bin/pdc\_server}
    \item \textbf{Redis}: \texttt{third\_party/redis/src/redis-server}
    \item \textbf{HDF5}: \texttt{h5py} library
    \item \textbf{\Cascade}: \texttt{cascade\_Code/cpp/build\_mpi/distributed\_bench}
\end{itemize}
\textbf{Never use simple Python file I/O labeled as these systems.}
\end{tcolorbox}

%==============================================================================
\subsection{Experimental Setup}
%==============================================================================

\textbf{Hardware.}
NERSC Perlmutter GPU nodes:
4$\times$ NVIDIA A100-40GB (160GB HBM per node),
AMD EPYC 7763 (64 cores), 256GB DDR4,
Slingshot-11 interconnect (200 Gb/s per NIC).
Lustre all-flash \texttt{\$SCRATCH}: 44PB capacity, 7.8 TB/s aggregate bandwidth.

\textbf{Comparison Systems.}
All systems are compiled from source in \texttt{third\_party/}:
\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Cascade}: C++ MPI distributed backend (\texttt{cascade\_Code/cpp/build\_mpi/distributed\_bench})
    \item \textbf{LMCache}: Real LMCache storage backend (requires torch, GPU node)
    \item \textbf{PDC}: Proactive Data Containers server + C API
    \item \textbf{Redis}: redis-server (requires libcudart.so.12, GPU node)
    \item \textbf{HDF5}: h5py with optional gzip compression
\end{enumerate}

%==============================================================================
\subsection{Cascade Unique Capabilities}
\label{sec:unique-capabilities}
%==============================================================================

\Cascade provides three capabilities that single-node systems fundamentally cannot implement:

\paragraph{1. Content-Addressed Deduplication.}
LMCache uses session-specific block IDs, causing the same system prompt to be stored $N$ times for $N$ sessions.
\Cascade uses SHA-256 content hashing, storing identical data only once.

\paragraph{2. Multi-Node SHM Scaling.}
LMCache is limited to one node's DDR4 bandwidth.
\Cascade aggregates SHM across all nodes via MPI.

\paragraph{3. Remote DRAM Fetch via Slingshot.}
When KV cache is on another node's SHM, LMCache must fall back to Lustre.
\Cascade fetches via Slingshot-11 at near-DRAM speed.

%==============================================================================
\subsection{Benchmark Methodology}
%==============================================================================

\textbf{Hot vs Cold Read.}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Hot read}: Data in SHM (\texttt{/dev/shm}) or OS page cache
    \item \textbf{Cold read}: Lustre read after \texttt{posix\_fadvise(fd, 0, size, POSIX\_FADV\_DONTNEED)}
\end{itemize}

\textbf{Cold Read Procedure.}
To ensure accurate cold read measurement:
\begin{lstlisting}[language=C]
int fd = open(path, O_RDONLY);
off_t size = lseek(fd, 0, SEEK_END);
posix_fadvise(fd, 0, size, POSIX_FADV_DONTNEED);
close(fd);
// Now read the file - goes directly to Lustre
\end{lstlisting}

%==============================================================================
\subsection{Expected Results}
%==============================================================================

\textbf{Hypothesis 1: Cascade dominates on hot reads.}
Direct mmap SHM access should be faster than file-based systems using OS page cache.

\textbf{Hypothesis 2: Cold reads converge to Lustre bandwidth.}
All per-file systems (LMCache, PDC, Redis, HDF5) should achieve similar cold read throughput
as they are limited by Lustre disk bandwidth.

\textbf{Hypothesis 3: Cascade's aggregated file is faster than per-file.}
Fewer \texttt{open()}/\texttt{stat()} calls, sequential I/O, Lustre striping.

%==============================================================================
\subsection{Results}
%==============================================================================

\begin{center}
\fbox{\parbox{0.9\columnwidth}{
\textbf{TODO}: Run real benchmarks with actual third\_party implementations.

Required experiments:
\begin{enumerate}
    \item 5-system comparison (Cascade, LMCache, PDC, Redis, HDF5)
    \item Hot read: All data in SHM/page cache
    \item Cold read: \texttt{posix\_fadvise(DONTNEED)} to drop page cache
    \item Scale: 4 nodes, 16 ranks, 16+ GB data
\end{enumerate}

All results must include:
\begin{itemize}
    \item SLURM Job ID for reproducibility
    \item Verification that actual third\_party code was used
    \item Environment details (nodes, ranks, GPUs)
\end{itemize}
}}
\end{center}

%==============================================================================
\subsection{Implementation Verification}
%==============================================================================

To ensure research integrity, all benchmarks must verify:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{LMCache}: Import from \texttt{lmcache.v1.storage\_backend.local\_disk\_backend.LocalDiskBackend}
    \item \textbf{PDC}: Use actual PDC C API (\texttt{PDCinit()}, \texttt{PDCobj\_create()}, etc.)
    \item \textbf{Redis}: Start \texttt{redis-server}, connect with \texttt{redis-py}
    \item \textbf{HDF5}: Use \texttt{h5py.File()} with actual HDF5 datasets
\end{enumerate}

\textbf{What NOT to do:}
\begin{lstlisting}[language=Python]
# WRONG - This is NOT LMCache!
class LMCacheStore:
    def put(self, block_id, data):
        with open(f"{block_id}.bin", 'wb') as f:
            f.write(data)  # Simple file I/O != LMCache
\end{lstlisting}

\textbf{Correct approach:}
\begin{lstlisting}[language=Python]
# CORRECT - Use actual LMCache
import sys
sys.path.insert(0, 'third_party/LMCache')
from lmcache.v1.storage_backend.local_disk_backend import LocalDiskBackend
backend = LocalDiskBackend(path="/tmp/lmcache", max_size=100*1024**3)
backend.put(block_id, data)
\end{lstlisting}

