# ðŸš€ Cascade V6: Distributed 5-Tier KV Cache for HPC-Scale LLM Inference

<p align="center">
  <img src="https://img.shields.io/badge/SC'26-Target-blue?style=for-the-badge" alt="SC'26"/>
  <img src="https://img.shields.io/badge/NERSC-Perlmutter-green?style=for-the-badge" alt="Perlmutter"/>
  <img src="https://img.shields.io/badge/Scale-8%20Nodes%20Verified-orange?style=for-the-badge" alt="Scale"/>
</p>

> **Core Objective:** Solving the **Memory Capacity & Bandwidth Wall** in LLM Serving by aggregating GPU/DRAM/NVMe resources across hundreds of HPC nodes.

---

## ðŸ† Key Features (Novelties for SC26)

Cascade V6 introduces a **Distributed 5-Tier Architecture** that unifies local and remote memory resources into a single, high-performance address space.

### 1. ðŸ§  Cross-Node Semantic Eviction (Novelty 1)
*   **Problem:** Traditional LRU evicts based on recent usage, ignoring semantic importance (e.g., System Prompts).
*   **Solution:** Global awareness of "Prefix Blocks". When memory pressure occurs, Cascade proactively evicts non-critical Suffix blocks while **protecting shared Prefix blocks** across the cluster.
*   **Impact:** Zero re-computation for popular system prompts (e.g., "You are a helpful assistant...").

### 2. ðŸŒ Distributed Content-Addressed Deduplication (Novelty 2)
*   **Problem:** Duplicate KV blocks consume massive memory in multi-tenant serving.
*   **Solution:** A **Global Distributed Hash Table (DHT)** using SHA256 content addressing. Identical blocks are stored **once globally** and referenced by all nodes.
*   **Impact:** **40%+ Memory Savings** on multi-turn dialogue workloads (ShareGPT).

### 3. ðŸ“ Locality-Aware Hierarchical Placement (Novelty 3)
*   **Problem:** Random placement leads to excessive network traffic.
*   **Solution:** Dynamic tracking of access frequency. "Hot" blocks are **auto-promoted** from Remote Tiers â†’ Local GPU/DRAM.
*   **Impact:** Minimizes network latency for frequently accessed tokens.

---

## ðŸ—ï¸ 5-Tier Memory Hierarchy

Cascade unifies heterogeneous storage into a single tier:

| Tier | Resource | Bandwidth (Measured) | Latency | Capacity (Per Node) |
| :--- | :--- | :--- | :--- | :--- |
| **Tier 1** | **Local GPU HBM** (A100) | **1,555 GB/s** | ~0.5 Î¼s | 160 GB (4x40GB) |
| **Tier 2** | **Local DRAM** (Pinned) | **160+ GB/s** | ~10 Î¼s | 256 GB |
| **Tier 3** | **Remote GPU** (NVLink/RDMA) | **22+ GB/s** | ~50 Î¼s | N Ã— 160 GB |
| **Tier 4** | **Remote DRAM** (RDMA) | **18 GB/s** | ~80 Î¼s | N Ã— 256 GB |
| **Tier 5** | **Lustre PFS** ($SCRATCH) | **1~3 GB/s** | ~ms | 44 PB (Shared) |

---

## ðŸ“Š Performance Results (Scaling Study)

**Platform:** NERSC Perlmutter (A100 GPU Nodes, Slingshot-11 Interconnect)
**Workload:** LLaMA-3 70B Block Size (160KB), 500 blocks/rank.

### Weak Scaling (1 to 8 Nodes)

| Nodes | Total GPUs | Agg. Write (GB/s) | Agg. Read (GB/s) | Speedup (Read) |
| :---: | :---: | :---: | :---: | :---: |
| **1** | 4 | 3.10 | 1.83 | 1.0x |
| **2** | 8 | 2.37 | 1.40 | 0.76x |
| **4** | 16 | 3.87 | 2.79 | 1.52x |
| **8** | 32 | **4.24** | **5.46** | **2.98x** |

> **Insight:** Efficient parallel scaling achieved. The architecture effectively masks the overhead of distributed metadata synchronization as node count increases.

### Feature Validation (2-Node)
*   **Dedup:** 20 Hits on shared system prompts (1.2MB saved).
*   **Protection:** 100% of Prefix blocks retained in upper tiers under pressure.
*   **Locality:** Hot remote blocks promoted to local GPU after 3 access hits.

---

## ðŸ”§ Getting Started

### Prerequisites
*   **Hardware:** NVIDIA GPUs (A100/H100), RDMA Network (Slingshot/Infiniband)
*   **Software:** CUDA Toolkit 12.x, MPI (Cray MPICH or OpenMPI), Python 3.10+

### Installation

```bash
# 1. Clone
git clone https://github.com/changjongkim/kcj-cascade-v6.git
cd kcj-cascade-v6

# 2. Build C++ Backend (MPI + CUDA)
./build_cpp.sh
```

### Usage (Python API)

Cascade V6 exposes a simple `DistributedStore` API that handles all tiering transparently.

```python
import cascade_cpp
from cascade_cpp import DistributedStore

# Configure
cfg = cascade_cpp.CascadeConfig()
cfg.dedup_enabled = True        # Novelty 2
cfg.semantic_eviction = True    # Novelty 1
cfg.locality_aware = True       # Novelty 3

# Initialize
store = DistributedStore(cfg)

# Put (Auto-Tiering + Compression)
block_id = "prefix_001"
data = generate_block()
store.put(block_id, data, is_prefix=True)

# Get (Locality-Aware Retrieval)
out_data = np.empty_like(data)
store.get(block_id, out_data)
```

---

## ðŸ“‚ Repository Structure

```
kcj-cascade-v6/
â”œâ”€â”€ cascade_Code/cpp/       # Core C++ Backend (CUDA + MPI)
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ scripts/            # SLURM Benchmarks (v6_scaling_*.slurm)
â”‚   â”œâ”€â”€ data_external/      # Real Workloads (ShareGPT, PG-19)
â”‚   â””â”€â”€ dataset_loader.py   # Data Loader Utility
â”œâ”€â”€ docs/                   # Documentation & Reports
â””â”€â”€ verify_*.py             # Functional Verification Scripts
```

---

## ðŸ“œ Citation

```bibtex
@inproceedings{kim2026cascade,
  title={Cascade V6: Distributed 5-Tier KV Cache for HPC-Scale LLM Inference},
  author={Kim, Sunggon and Kim, Changjong},
  year={2026},
  note={Targeting SC26}
}
```

---

**Contact:** Sunggon Kim (sgkim@lbl.gov)
